{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e3f8b4",
   "metadata": {},
   "source": [
    "# CNN 실습\n",
    "\n",
    "이번 시간에는 대장 조직 데이터를 활용하여 앞서 배운 CNN모델을 만들어보겠습니다.\n",
    "\n",
    "`Colorectal histology` 데이터는 대장의 조직 모양과 8가지 클래스 정보가 들어있습니다.\n",
    "\n",
    "더 고화질의 데이터를 활용할 수 있지만, 원활한 실습을 위해 28 * 28 픽셀 데이터를 사용하여 분류해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "406a8857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers \n",
    "# from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4a85c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d47f43",
   "metadata": {},
   "source": [
    "# 파일로드, 정규화, reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "13fed9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0000</th>\n",
       "      <th>pixel0001</th>\n",
       "      <th>pixel0002</th>\n",
       "      <th>pixel0003</th>\n",
       "      <th>pixel0004</th>\n",
       "      <th>pixel0005</th>\n",
       "      <th>pixel0006</th>\n",
       "      <th>pixel0007</th>\n",
       "      <th>pixel0008</th>\n",
       "      <th>pixel0009</th>\n",
       "      <th>pixel0010</th>\n",
       "      <th>pixel0011</th>\n",
       "      <th>pixel0012</th>\n",
       "      <th>pixel0013</th>\n",
       "      <th>pixel0014</th>\n",
       "      <th>pixel0015</th>\n",
       "      <th>pixel0016</th>\n",
       "      <th>pixel0017</th>\n",
       "      <th>pixel0018</th>\n",
       "      <th>pixel0019</th>\n",
       "      <th>pixel0020</th>\n",
       "      <th>pixel0021</th>\n",
       "      <th>pixel0022</th>\n",
       "      <th>pixel0023</th>\n",
       "      <th>pixel0024</th>\n",
       "      <th>pixel0025</th>\n",
       "      <th>pixel0026</th>\n",
       "      <th>pixel0027</th>\n",
       "      <th>pixel0028</th>\n",
       "      <th>pixel0029</th>\n",
       "      <th>pixel0030</th>\n",
       "      <th>pixel0031</th>\n",
       "      <th>pixel0032</th>\n",
       "      <th>pixel0033</th>\n",
       "      <th>pixel0034</th>\n",
       "      <th>pixel0035</th>\n",
       "      <th>pixel0036</th>\n",
       "      <th>pixel0037</th>\n",
       "      <th>pixel0038</th>\n",
       "      <th>pixel0039</th>\n",
       "      <th>pixel0040</th>\n",
       "      <th>pixel0041</th>\n",
       "      <th>pixel0042</th>\n",
       "      <th>pixel0043</th>\n",
       "      <th>pixel0044</th>\n",
       "      <th>pixel0045</th>\n",
       "      <th>pixel0046</th>\n",
       "      <th>pixel0047</th>\n",
       "      <th>pixel0048</th>\n",
       "      <th>pixel0049</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel0735</th>\n",
       "      <th>pixel0736</th>\n",
       "      <th>pixel0737</th>\n",
       "      <th>pixel0738</th>\n",
       "      <th>pixel0739</th>\n",
       "      <th>pixel0740</th>\n",
       "      <th>pixel0741</th>\n",
       "      <th>pixel0742</th>\n",
       "      <th>pixel0743</th>\n",
       "      <th>pixel0744</th>\n",
       "      <th>pixel0745</th>\n",
       "      <th>pixel0746</th>\n",
       "      <th>pixel0747</th>\n",
       "      <th>pixel0748</th>\n",
       "      <th>pixel0749</th>\n",
       "      <th>pixel0750</th>\n",
       "      <th>pixel0751</th>\n",
       "      <th>pixel0752</th>\n",
       "      <th>pixel0753</th>\n",
       "      <th>pixel0754</th>\n",
       "      <th>pixel0755</th>\n",
       "      <th>pixel0756</th>\n",
       "      <th>pixel0757</th>\n",
       "      <th>pixel0758</th>\n",
       "      <th>pixel0759</th>\n",
       "      <th>pixel0760</th>\n",
       "      <th>pixel0761</th>\n",
       "      <th>pixel0762</th>\n",
       "      <th>pixel0763</th>\n",
       "      <th>pixel0764</th>\n",
       "      <th>pixel0765</th>\n",
       "      <th>pixel0766</th>\n",
       "      <th>pixel0767</th>\n",
       "      <th>pixel0768</th>\n",
       "      <th>pixel0769</th>\n",
       "      <th>pixel0770</th>\n",
       "      <th>pixel0771</th>\n",
       "      <th>pixel0772</th>\n",
       "      <th>pixel0773</th>\n",
       "      <th>pixel0774</th>\n",
       "      <th>pixel0775</th>\n",
       "      <th>pixel0776</th>\n",
       "      <th>pixel0777</th>\n",
       "      <th>pixel0778</th>\n",
       "      <th>pixel0779</th>\n",
       "      <th>pixel0780</th>\n",
       "      <th>pixel0781</th>\n",
       "      <th>pixel0782</th>\n",
       "      <th>pixel0783</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>110</td>\n",
       "      <td>154</td>\n",
       "      <td>160</td>\n",
       "      <td>95</td>\n",
       "      <td>44</td>\n",
       "      <td>139</td>\n",
       "      <td>184</td>\n",
       "      <td>164</td>\n",
       "      <td>160</td>\n",
       "      <td>115</td>\n",
       "      <td>119</td>\n",
       "      <td>197</td>\n",
       "      <td>101</td>\n",
       "      <td>151</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>187</td>\n",
       "      <td>153</td>\n",
       "      <td>92</td>\n",
       "      <td>47</td>\n",
       "      <td>21</td>\n",
       "      <td>50</td>\n",
       "      <td>130</td>\n",
       "      <td>103</td>\n",
       "      <td>110</td>\n",
       "      <td>143</td>\n",
       "      <td>80</td>\n",
       "      <td>94</td>\n",
       "      <td>130</td>\n",
       "      <td>138</td>\n",
       "      <td>209</td>\n",
       "      <td>181</td>\n",
       "      <td>53</td>\n",
       "      <td>90</td>\n",
       "      <td>174</td>\n",
       "      <td>158</td>\n",
       "      <td>143</td>\n",
       "      <td>116</td>\n",
       "      <td>69</td>\n",
       "      <td>109</td>\n",
       "      <td>140</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>191</td>\n",
       "      <td>165</td>\n",
       "      <td>125</td>\n",
       "      <td>91</td>\n",
       "      <td>125</td>\n",
       "      <td>104</td>\n",
       "      <td>...</td>\n",
       "      <td>140</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>160</td>\n",
       "      <td>72</td>\n",
       "      <td>53</td>\n",
       "      <td>121</td>\n",
       "      <td>47</td>\n",
       "      <td>59</td>\n",
       "      <td>141</td>\n",
       "      <td>150</td>\n",
       "      <td>118</td>\n",
       "      <td>103</td>\n",
       "      <td>117</td>\n",
       "      <td>100</td>\n",
       "      <td>71</td>\n",
       "      <td>128</td>\n",
       "      <td>102</td>\n",
       "      <td>110</td>\n",
       "      <td>141</td>\n",
       "      <td>139</td>\n",
       "      <td>76</td>\n",
       "      <td>98</td>\n",
       "      <td>115</td>\n",
       "      <td>124</td>\n",
       "      <td>176</td>\n",
       "      <td>180</td>\n",
       "      <td>147</td>\n",
       "      <td>121</td>\n",
       "      <td>101</td>\n",
       "      <td>121</td>\n",
       "      <td>138</td>\n",
       "      <td>74</td>\n",
       "      <td>95</td>\n",
       "      <td>163</td>\n",
       "      <td>75</td>\n",
       "      <td>62</td>\n",
       "      <td>128</td>\n",
       "      <td>138</td>\n",
       "      <td>128</td>\n",
       "      <td>103</td>\n",
       "      <td>73</td>\n",
       "      <td>72</td>\n",
       "      <td>75</td>\n",
       "      <td>152</td>\n",
       "      <td>130</td>\n",
       "      <td>96</td>\n",
       "      <td>133</td>\n",
       "      <td>159</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>66</td>\n",
       "      <td>69</td>\n",
       "      <td>76</td>\n",
       "      <td>80</td>\n",
       "      <td>57</td>\n",
       "      <td>46</td>\n",
       "      <td>67</td>\n",
       "      <td>90</td>\n",
       "      <td>77</td>\n",
       "      <td>82</td>\n",
       "      <td>51</td>\n",
       "      <td>39</td>\n",
       "      <td>62</td>\n",
       "      <td>84</td>\n",
       "      <td>78</td>\n",
       "      <td>82</td>\n",
       "      <td>64</td>\n",
       "      <td>81</td>\n",
       "      <td>79</td>\n",
       "      <td>46</td>\n",
       "      <td>76</td>\n",
       "      <td>93</td>\n",
       "      <td>71</td>\n",
       "      <td>41</td>\n",
       "      <td>67</td>\n",
       "      <td>106</td>\n",
       "      <td>38</td>\n",
       "      <td>58</td>\n",
       "      <td>52</td>\n",
       "      <td>67</td>\n",
       "      <td>83</td>\n",
       "      <td>73</td>\n",
       "      <td>53</td>\n",
       "      <td>55</td>\n",
       "      <td>50</td>\n",
       "      <td>43</td>\n",
       "      <td>46</td>\n",
       "      <td>73</td>\n",
       "      <td>50</td>\n",
       "      <td>60</td>\n",
       "      <td>64</td>\n",
       "      <td>44</td>\n",
       "      <td>65</td>\n",
       "      <td>82</td>\n",
       "      <td>77</td>\n",
       "      <td>92</td>\n",
       "      <td>50</td>\n",
       "      <td>48</td>\n",
       "      <td>86</td>\n",
       "      <td>...</td>\n",
       "      <td>74</td>\n",
       "      <td>77</td>\n",
       "      <td>81</td>\n",
       "      <td>83</td>\n",
       "      <td>84</td>\n",
       "      <td>70</td>\n",
       "      <td>55</td>\n",
       "      <td>54</td>\n",
       "      <td>57</td>\n",
       "      <td>56</td>\n",
       "      <td>64</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>71</td>\n",
       "      <td>81</td>\n",
       "      <td>90</td>\n",
       "      <td>80</td>\n",
       "      <td>74</td>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>74</td>\n",
       "      <td>79</td>\n",
       "      <td>77</td>\n",
       "      <td>64</td>\n",
       "      <td>63</td>\n",
       "      <td>57</td>\n",
       "      <td>56</td>\n",
       "      <td>51</td>\n",
       "      <td>56</td>\n",
       "      <td>64</td>\n",
       "      <td>76</td>\n",
       "      <td>79</td>\n",
       "      <td>76</td>\n",
       "      <td>82</td>\n",
       "      <td>88</td>\n",
       "      <td>84</td>\n",
       "      <td>64</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>57</td>\n",
       "      <td>58</td>\n",
       "      <td>65</td>\n",
       "      <td>74</td>\n",
       "      <td>80</td>\n",
       "      <td>81</td>\n",
       "      <td>83</td>\n",
       "      <td>77</td>\n",
       "      <td>75</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>127</td>\n",
       "      <td>137</td>\n",
       "      <td>121</td>\n",
       "      <td>140</td>\n",
       "      <td>170</td>\n",
       "      <td>111</td>\n",
       "      <td>128</td>\n",
       "      <td>117</td>\n",
       "      <td>60</td>\n",
       "      <td>105</td>\n",
       "      <td>90</td>\n",
       "      <td>53</td>\n",
       "      <td>81</td>\n",
       "      <td>117</td>\n",
       "      <td>171</td>\n",
       "      <td>148</td>\n",
       "      <td>96</td>\n",
       "      <td>109</td>\n",
       "      <td>105</td>\n",
       "      <td>127</td>\n",
       "      <td>225</td>\n",
       "      <td>244</td>\n",
       "      <td>151</td>\n",
       "      <td>139</td>\n",
       "      <td>128</td>\n",
       "      <td>153</td>\n",
       "      <td>160</td>\n",
       "      <td>112</td>\n",
       "      <td>135</td>\n",
       "      <td>130</td>\n",
       "      <td>106</td>\n",
       "      <td>117</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>109</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>91</td>\n",
       "      <td>116</td>\n",
       "      <td>145</td>\n",
       "      <td>131</td>\n",
       "      <td>117</td>\n",
       "      <td>119</td>\n",
       "      <td>121</td>\n",
       "      <td>136</td>\n",
       "      <td>124</td>\n",
       "      <td>87</td>\n",
       "      <td>85</td>\n",
       "      <td>171</td>\n",
       "      <td>200</td>\n",
       "      <td>...</td>\n",
       "      <td>210</td>\n",
       "      <td>139</td>\n",
       "      <td>153</td>\n",
       "      <td>147</td>\n",
       "      <td>140</td>\n",
       "      <td>120</td>\n",
       "      <td>93</td>\n",
       "      <td>141</td>\n",
       "      <td>121</td>\n",
       "      <td>79</td>\n",
       "      <td>80</td>\n",
       "      <td>109</td>\n",
       "      <td>149</td>\n",
       "      <td>125</td>\n",
       "      <td>152</td>\n",
       "      <td>146</td>\n",
       "      <td>149</td>\n",
       "      <td>113</td>\n",
       "      <td>167</td>\n",
       "      <td>146</td>\n",
       "      <td>97</td>\n",
       "      <td>108</td>\n",
       "      <td>59</td>\n",
       "      <td>110</td>\n",
       "      <td>132</td>\n",
       "      <td>69</td>\n",
       "      <td>125</td>\n",
       "      <td>159</td>\n",
       "      <td>179</td>\n",
       "      <td>109</td>\n",
       "      <td>122</td>\n",
       "      <td>143</td>\n",
       "      <td>119</td>\n",
       "      <td>127</td>\n",
       "      <td>115</td>\n",
       "      <td>128</td>\n",
       "      <td>109</td>\n",
       "      <td>91</td>\n",
       "      <td>89</td>\n",
       "      <td>69</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>143</td>\n",
       "      <td>119</td>\n",
       "      <td>148</td>\n",
       "      <td>140</td>\n",
       "      <td>193</td>\n",
       "      <td>146</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "      <td>90</td>\n",
       "      <td>101</td>\n",
       "      <td>106</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>99</td>\n",
       "      <td>66</td>\n",
       "      <td>63</td>\n",
       "      <td>91</td>\n",
       "      <td>95</td>\n",
       "      <td>90</td>\n",
       "      <td>87</td>\n",
       "      <td>120</td>\n",
       "      <td>82</td>\n",
       "      <td>52</td>\n",
       "      <td>79</td>\n",
       "      <td>100</td>\n",
       "      <td>135</td>\n",
       "      <td>92</td>\n",
       "      <td>65</td>\n",
       "      <td>113</td>\n",
       "      <td>84</td>\n",
       "      <td>82</td>\n",
       "      <td>92</td>\n",
       "      <td>111</td>\n",
       "      <td>125</td>\n",
       "      <td>92</td>\n",
       "      <td>84</td>\n",
       "      <td>82</td>\n",
       "      <td>91</td>\n",
       "      <td>100</td>\n",
       "      <td>112</td>\n",
       "      <td>97</td>\n",
       "      <td>95</td>\n",
       "      <td>85</td>\n",
       "      <td>60</td>\n",
       "      <td>78</td>\n",
       "      <td>95</td>\n",
       "      <td>103</td>\n",
       "      <td>81</td>\n",
       "      <td>108</td>\n",
       "      <td>112</td>\n",
       "      <td>50</td>\n",
       "      <td>60</td>\n",
       "      <td>90</td>\n",
       "      <td>110</td>\n",
       "      <td>127</td>\n",
       "      <td>77</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>105</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "      <td>100</td>\n",
       "      <td>108</td>\n",
       "      <td>128</td>\n",
       "      <td>144</td>\n",
       "      <td>83</td>\n",
       "      <td>99</td>\n",
       "      <td>133</td>\n",
       "      <td>160</td>\n",
       "      <td>143</td>\n",
       "      <td>119</td>\n",
       "      <td>98</td>\n",
       "      <td>115</td>\n",
       "      <td>88</td>\n",
       "      <td>76</td>\n",
       "      <td>128</td>\n",
       "      <td>91</td>\n",
       "      <td>79</td>\n",
       "      <td>78</td>\n",
       "      <td>128</td>\n",
       "      <td>204</td>\n",
       "      <td>180</td>\n",
       "      <td>79</td>\n",
       "      <td>70</td>\n",
       "      <td>99</td>\n",
       "      <td>76</td>\n",
       "      <td>87</td>\n",
       "      <td>107</td>\n",
       "      <td>97</td>\n",
       "      <td>90</td>\n",
       "      <td>97</td>\n",
       "      <td>92</td>\n",
       "      <td>86</td>\n",
       "      <td>113</td>\n",
       "      <td>85</td>\n",
       "      <td>123</td>\n",
       "      <td>148</td>\n",
       "      <td>154</td>\n",
       "      <td>131</td>\n",
       "      <td>109</td>\n",
       "      <td>97</td>\n",
       "      <td>102</td>\n",
       "      <td>71</td>\n",
       "      <td>93</td>\n",
       "      <td>120</td>\n",
       "      <td>84</td>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>153</td>\n",
       "      <td>141</td>\n",
       "      <td>121</td>\n",
       "      <td>132</td>\n",
       "      <td>110</td>\n",
       "      <td>131</td>\n",
       "      <td>119</td>\n",
       "      <td>99</td>\n",
       "      <td>101</td>\n",
       "      <td>91</td>\n",
       "      <td>85</td>\n",
       "      <td>138</td>\n",
       "      <td>159</td>\n",
       "      <td>147</td>\n",
       "      <td>159</td>\n",
       "      <td>150</td>\n",
       "      <td>105</td>\n",
       "      <td>109</td>\n",
       "      <td>131</td>\n",
       "      <td>112</td>\n",
       "      <td>134</td>\n",
       "      <td>158</td>\n",
       "      <td>106</td>\n",
       "      <td>109</td>\n",
       "      <td>163</td>\n",
       "      <td>148</td>\n",
       "      <td>121</td>\n",
       "      <td>175</td>\n",
       "      <td>152</td>\n",
       "      <td>176</td>\n",
       "      <td>129</td>\n",
       "      <td>144</td>\n",
       "      <td>140</td>\n",
       "      <td>132</td>\n",
       "      <td>135</td>\n",
       "      <td>93</td>\n",
       "      <td>97</td>\n",
       "      <td>73</td>\n",
       "      <td>97</td>\n",
       "      <td>144</td>\n",
       "      <td>160</td>\n",
       "      <td>158</td>\n",
       "      <td>153</td>\n",
       "      <td>147</td>\n",
       "      <td>139</td>\n",
       "      <td>166</td>\n",
       "      <td>170</td>\n",
       "      <td>113</td>\n",
       "      <td>124</td>\n",
       "      <td>149</td>\n",
       "      <td>...</td>\n",
       "      <td>101</td>\n",
       "      <td>85</td>\n",
       "      <td>124</td>\n",
       "      <td>121</td>\n",
       "      <td>48</td>\n",
       "      <td>45</td>\n",
       "      <td>143</td>\n",
       "      <td>122</td>\n",
       "      <td>128</td>\n",
       "      <td>158</td>\n",
       "      <td>108</td>\n",
       "      <td>90</td>\n",
       "      <td>107</td>\n",
       "      <td>121</td>\n",
       "      <td>123</td>\n",
       "      <td>148</td>\n",
       "      <td>165</td>\n",
       "      <td>189</td>\n",
       "      <td>174</td>\n",
       "      <td>145</td>\n",
       "      <td>171</td>\n",
       "      <td>118</td>\n",
       "      <td>150</td>\n",
       "      <td>98</td>\n",
       "      <td>114</td>\n",
       "      <td>200</td>\n",
       "      <td>171</td>\n",
       "      <td>112</td>\n",
       "      <td>110</td>\n",
       "      <td>137</td>\n",
       "      <td>109</td>\n",
       "      <td>114</td>\n",
       "      <td>85</td>\n",
       "      <td>106</td>\n",
       "      <td>137</td>\n",
       "      <td>141</td>\n",
       "      <td>131</td>\n",
       "      <td>103</td>\n",
       "      <td>107</td>\n",
       "      <td>134</td>\n",
       "      <td>117</td>\n",
       "      <td>121</td>\n",
       "      <td>136</td>\n",
       "      <td>178</td>\n",
       "      <td>192</td>\n",
       "      <td>210</td>\n",
       "      <td>189</td>\n",
       "      <td>149</td>\n",
       "      <td>155</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel0000  pixel0001  pixel0002  pixel0003  pixel0004  pixel0005  \\\n",
       "0        101        110        154        160         95         44   \n",
       "1         67         66         69         76         80         57   \n",
       "2        127        137        121        140        170        111   \n",
       "3         80         90        101        106        120        100   \n",
       "4        153        141        121        132        110        131   \n",
       "\n",
       "   pixel0006  pixel0007  pixel0008  pixel0009  pixel0010  pixel0011  \\\n",
       "0        139        184        164        160        115        119   \n",
       "1         46         67         90         77         82         51   \n",
       "2        128        117         60        105         90         53   \n",
       "3         99         66         63         91         95         90   \n",
       "4        119         99        101         91         85        138   \n",
       "\n",
       "   pixel0012  pixel0013  pixel0014  pixel0015  pixel0016  pixel0017  \\\n",
       "0        197        101        151        186        186        187   \n",
       "1         39         62         84         78         82         64   \n",
       "2         81        117        171        148         96        109   \n",
       "3         87        120         82         52         79        100   \n",
       "4        159        147        159        150        105        109   \n",
       "\n",
       "   pixel0018  pixel0019  pixel0020  pixel0021  pixel0022  pixel0023  \\\n",
       "0        153         92         47         21         50        130   \n",
       "1         81         79         46         76         93         71   \n",
       "2        105        127        225        244        151        139   \n",
       "3        135         92         65        113         84         82   \n",
       "4        131        112        134        158        106        109   \n",
       "\n",
       "   pixel0024  pixel0025  pixel0026  pixel0027  pixel0028  pixel0029  \\\n",
       "0        103        110        143         80         94        130   \n",
       "1         41         67        106         38         58         52   \n",
       "2        128        153        160        112        135        130   \n",
       "3         92        111        125         92         84         82   \n",
       "4        163        148        121        175        152        176   \n",
       "\n",
       "   pixel0030  pixel0031  pixel0032  pixel0033  pixel0034  pixel0035  \\\n",
       "0        138        209        181         53         90        174   \n",
       "1         67         83         73         53         55         50   \n",
       "2        106        117        105        108        109        116   \n",
       "3         91        100        112         97         95         85   \n",
       "4        129        144        140        132        135         93   \n",
       "\n",
       "   pixel0036  pixel0037  pixel0038  pixel0039  pixel0040  pixel0041  \\\n",
       "0        158        143        116         69        109        140   \n",
       "1         43         46         73         50         60         64   \n",
       "2         74         91        116        145        131        117   \n",
       "3         60         78         95        103         81        108   \n",
       "4         97         73         97        144        160        158   \n",
       "\n",
       "   pixel0042  pixel0043  pixel0044  pixel0045  pixel0046  pixel0047  \\\n",
       "0        156        156        191        165        125         91   \n",
       "1         44         65         82         77         92         50   \n",
       "2        119        121        136        124         87         85   \n",
       "3        112         50         60         90        110        127   \n",
       "4        153        147        139        166        170        113   \n",
       "\n",
       "   pixel0048  pixel0049  ...  pixel0735  pixel0736  pixel0737  pixel0738  \\\n",
       "0        125        104  ...        140        133        133        160   \n",
       "1         48         86  ...         74         77         81         83   \n",
       "2        171        200  ...        210        139        153        147   \n",
       "3         77         76  ...        105         95         93        100   \n",
       "4        124        149  ...        101         85        124        121   \n",
       "\n",
       "   pixel0739  pixel0740  pixel0741  pixel0742  pixel0743  pixel0744  \\\n",
       "0         72         53        121         47         59        141   \n",
       "1         84         70         55         54         57         56   \n",
       "2        140        120         93        141        121         79   \n",
       "3        108        128        144         83         99        133   \n",
       "4         48         45        143        122        128        158   \n",
       "\n",
       "   pixel0745  pixel0746  pixel0747  pixel0748  pixel0749  pixel0750  \\\n",
       "0        150        118        103        117        100         71   \n",
       "1         64         75         75         71         81         90   \n",
       "2         80        109        149        125        152        146   \n",
       "3        160        143        119         98        115         88   \n",
       "4        108         90        107        121        123        148   \n",
       "\n",
       "   pixel0751  pixel0752  pixel0753  pixel0754  pixel0755  pixel0756  \\\n",
       "0        128        102        110        141        139         76   \n",
       "1         80         74         76         76         74         79   \n",
       "2        149        113        167        146         97        108   \n",
       "3         76        128         91         79         78        128   \n",
       "4        165        189        174        145        171        118   \n",
       "\n",
       "   pixel0757  pixel0758  pixel0759  pixel0760  pixel0761  pixel0762  \\\n",
       "0         98        115        124        176        180        147   \n",
       "1         77         64         63         57         56         51   \n",
       "2         59        110        132         69        125        159   \n",
       "3        204        180         79         70         99         76   \n",
       "4        150         98        114        200        171        112   \n",
       "\n",
       "   pixel0763  pixel0764  pixel0765  pixel0766  pixel0767  pixel0768  \\\n",
       "0        121        101        121        138         74         95   \n",
       "1         56         64         76         79         76         82   \n",
       "2        179        109        122        143        119        127   \n",
       "3         87        107         97         90         97         92   \n",
       "4        110        137        109        114         85        106   \n",
       "\n",
       "   pixel0769  pixel0770  pixel0771  pixel0772  pixel0773  pixel0774  \\\n",
       "0        163         75         62        128        138        128   \n",
       "1         88         84         64         58         58         57   \n",
       "2        115        128        109         91         89         69   \n",
       "3         86        113         85        123        148        154   \n",
       "4        137        141        131        103        107        134   \n",
       "\n",
       "   pixel0775  pixel0776  pixel0777  pixel0778  pixel0779  pixel0780  \\\n",
       "0        103         73         72         75        152        130   \n",
       "1         58         65         74         80         81         83   \n",
       "2         90        100        143        119        148        140   \n",
       "3        131        109         97        102         71         93   \n",
       "4        117        121        136        178        192        210   \n",
       "\n",
       "   pixel0781  pixel0782  pixel0783  label  \n",
       "0         96        133        159      2  \n",
       "1         77         75         73      2  \n",
       "2        193        146         97      2  \n",
       "3        120         84         62      2  \n",
       "4        189        149        155      2  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('hmnist_28_28_L.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8a86cef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 785)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "410211d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    625\n",
       "5    625\n",
       "7    625\n",
       "6    625\n",
       "8    625\n",
       "1    625\n",
       "4    625\n",
       "3    625\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a17b6c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('label',axis=1)\n",
    "Y = data['label']\n",
    "\n",
    "X = X/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2770acd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 784)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f2c7fc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.values.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dce15ab9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 28, 28, 1)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faa7705",
   "metadata": {},
   "source": [
    "# 이미지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7bb8fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8f75c477",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[0], cmap='Gray')\n",
    "plt.title(Y[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6b28d8",
   "metadata": {},
   "source": [
    "# label 정보 encoding\n",
    "\n",
    "- 숫자로 되어있는 label 정보를 one-hot-encoding으로 변환하여 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b2322f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding \n",
    "from tensorflow.keras.utils import to_categorical # convert to one-hot-encoding for better results\n",
    "Y = to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "747bd8e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[1], cmap='Gray')\n",
    "plt.title(Y[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d39fb0",
   "metadata": {},
   "source": [
    "# train-test 분리\n",
    "\n",
    "train - test set을 분리하고, train 데이터 내에서 validation set도 분리하여 활용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bd6a4dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9a8d8d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.2, stratify=Y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2fde2732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200, 28, 28, 1)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "78e2729d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 28, 28, 1)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "daa57588",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 28, 28, 1)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2c1dcdc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200, 9)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8d7b43",
   "metadata": {},
   "source": [
    "# 모델 만들기\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    앞 시간에 배운 내용을 활용해서 각자 CNN 모델을 만들고 학습을 진행한 뒤 test accuracy를 구해보세요\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bd466c",
   "metadata": {},
   "source": [
    "학습 이후 예측 값을 출력해보면 각 클래스에 속할 확률 값이 들어있는 array가 출력됩니다.\n",
    "\n",
    "이 중 가장 큰 값이 있는 컬럼을 최종 결과로 도출할 것이므로 argmax를 사용하여 가장 큰 값의 인덱스를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2bb9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d9d4c80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_class = np.argmax(Y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "cc352e0e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.795"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(Y_test, axis=1), Y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ce8df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e110b19",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "\n",
    "이미지를 분류하는 CNN 모델도 일반적인 머신러닝 모델과 마찬가지로 overfitting(과적합)에 유의해야합니다. \n",
    "\n",
    "CNN 모델의 성능을 높이면서 과적합을 극복할 수 있는 근본적인 해결책은 학습 데이터의 다양성을 늘리는 것입니다.\n",
    "\n",
    "그러나 학습 데이터를 아무리 많이 수집하더라도 수집된 데이터 내에는 편향된 데이터가 존재할 수 밖에 없습니다.\n",
    "\n",
    "예를 들어 강아지 이미지를 수집하다보면 아래처럼 밝은 곳에서 찍은 사진이 많이 수집됩니다.<br>\n",
    "\n",
    "<img src=\"https://blog.kakaocdn.net/dn/bQCM5T/btrIdc5kcBx/wj5SFfUOlcaGQbMkuDaGK1/img.jpg\" width=\"150\" heigh=\"100\" align='left'/>\n",
    "<img src=\"https://blog.kakaocdn.net/dn/XWklf/btrH49okHPv/xkZpFR5oBt5GiJToWWjQt0/img.jpg\" width=\"150\" heigh=\"100\" align='left'/>\n",
    "<img src=\"https://blog.kakaocdn.net/dn/cqDCW9/btrIcHdWeVD/R9rWs9MtB4VAztq1Au2oXk/img.jpg\" width=\"113\" heigh=\"100\" align='left'/>\n",
    "<img src=\"https://blog.kakaocdn.net/dn/bLc1Cl/btrIbL8FLqN/JlKbsCXaW2yD5i2cMjc9tk/img.jpg\" width=\"113\" heigh=\"100\" align='left'/>\n",
    "<br><br><br><br><br><br><br><br>\n",
    "\n",
    "이때 아래 사진처럼 test 데이터로 어두운 곳에서 찍힌 강아지 사진이 들어오면 모델은 새로운 유형의 사진으로 인식하여 맞추기 어려워합니다.\n",
    "\n",
    "<img src=\"https://blog.kakaocdn.net/dn/ohfkL/btrIej337KW/qAlaZ03fip6zxKUcGyZT11/img.jpg\" width=\"113\" heigh=\"100\" align='left'/>\n",
    "<br><br><br><br><br><br><br><br><br>\n",
    "따라서 데이터를 아무리 많이 모으더라도 항상 데이터는 부족하다고 볼 수 있습니다.\n",
    "\n",
    "시간과 비용을 적게 소모하면서 학습 데이터의 다양성을 늘리기 위해 나온 것이 Data Augmentation 입니다.\n",
    "\n",
    "하나의 원본 이미지를 다양한 버전으로 만들어서 학습시킵니다.\n",
    "\n",
    "이때 자주 사용되는 기법은 다음과 같습니다.\n",
    "\n",
    "- Flipping\n",
    "- Rotating\n",
    "- Cropping\n",
    "- Colour jittering\n",
    "- Edge Enhancement\n",
    "- Fancy PCA\n",
    "- Mixup\n",
    "\n",
    "다양한 방법이 있습니다.\n",
    "\n",
    "Data Augmentation 할 때 주의할 점은 원래 데이터의 의미가 달라지면 안되는 점 입니다. 예를 들어 숫자 6을 뒤집으면 9가 되면서 의미가 변질되기 때문에 주의해야합니다.\n",
    "\n",
    "따라서 데이터 셋마다 적절한 기법을 선택하는 것이 중요합니다.\n",
    "\n",
    "Tensorflow.keras의 ImageDataGenerator 는 Data Augmentation을 대신 해주는 함수입니다.\n",
    "\n",
    "ImageDataGenerator 를 사용하면 객체 생성 이후 flow 메소드에 배치사이즈, X, Y 설정을 인자로 넣어줘야합니다.\n",
    "\n",
    "앞서 배운 CNN 모델과 다르게 인자를 추가하는 이유는, 이미지 사이즈가 커질수록 한 번에 이미지를 CPU, GPU에 올릴 수 없기 때문에 배치 사이즈 크기 설정 같은 처리를 미리 ImageDataGenerator가 처리해야하기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac4fe1e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    앞서 만든 모델을 학습시키는 과정에 data augmentation을 적용해보겠습니다.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d117d7",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a94a5b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "96672531",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=0.5, \n",
    "        zoom_range = 0.5, \n",
    "        width_shift_range=0.5,  \n",
    "        height_shift_range=0.5, \n",
    "        horizontal_flip=True, \n",
    "        vertical_flip=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ff5a80b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kjy\\anaconda3\\envs\\env_keras\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "16/16 [==============================] - 3s 144ms/step - loss: 2.1284 - accuracy: 0.1263 - val_loss: 2.0296 - val_accuracy: 0.1250\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 1.9391 - accuracy: 0.1436 - val_loss: 1.8588 - val_accuracy: 0.2100\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 1.6732 - accuracy: 0.2779 - val_loss: 1.5193 - val_accuracy: 0.3512\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.4178 - accuracy: 0.3563 - val_loss: 1.4108 - val_accuracy: 0.3462\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 1.2852 - accuracy: 0.3921 - val_loss: 1.3092 - val_accuracy: 0.4450\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 2s 122ms/step - loss: 1.2970 - accuracy: 0.4189 - val_loss: 1.4085 - val_accuracy: 0.4350\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 1.2574 - accuracy: 0.4513 - val_loss: 1.2430 - val_accuracy: 0.4938\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 1.2045 - accuracy: 0.4824 - val_loss: 1.2390 - val_accuracy: 0.4913\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 1.2543 - accuracy: 0.4252 - val_loss: 1.4322 - val_accuracy: 0.4437\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 1.1656 - accuracy: 0.5170 - val_loss: 1.4861 - val_accuracy: 0.3212\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 1.1745 - accuracy: 0.5017 - val_loss: 1.2343 - val_accuracy: 0.4863\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 1.2139 - accuracy: 0.4756 - val_loss: 1.2408 - val_accuracy: 0.5450\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 1.1283 - accuracy: 0.5232 - val_loss: 1.1803 - val_accuracy: 0.5400\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 1.0809 - accuracy: 0.5634 - val_loss: 1.1443 - val_accuracy: 0.5487\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 1.1449 - accuracy: 0.5145 - val_loss: 1.3021 - val_accuracy: 0.4650\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 1.1101 - accuracy: 0.5174 - val_loss: 1.2905 - val_accuracy: 0.5200\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 1.1019 - accuracy: 0.5298 - val_loss: 1.1748 - val_accuracy: 0.5125\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 1.0768 - accuracy: 0.5502 - val_loss: 1.1224 - val_accuracy: 0.5487\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 1.0974 - accuracy: 0.5378 - val_loss: 1.0815 - val_accuracy: 0.5113\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 1.2347 - accuracy: 0.4735 - val_loss: 1.0726 - val_accuracy: 0.6112\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 1.1052 - accuracy: 0.5335 - val_loss: 1.3486 - val_accuracy: 0.4825\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 1.0790 - accuracy: 0.5357 - val_loss: 1.3135 - val_accuracy: 0.4162\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 1.1721 - accuracy: 0.4788 - val_loss: 1.3171 - val_accuracy: 0.5100\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 1.1353 - accuracy: 0.5285 - val_loss: 1.1042 - val_accuracy: 0.5962\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 1.0734 - accuracy: 0.5688 - val_loss: 1.3834 - val_accuracy: 0.4200\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 1.1159 - accuracy: 0.5395 - val_loss: 1.0745 - val_accuracy: 0.6137\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 1.0761 - accuracy: 0.5669 - val_loss: 1.1541 - val_accuracy: 0.5700\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 1.0547 - accuracy: 0.5651 - val_loss: 1.0104 - val_accuracy: 0.6288\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 1.0972 - accuracy: 0.5354 - val_loss: 1.2149 - val_accuracy: 0.4500\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 1.0702 - accuracy: 0.5504 - val_loss: 1.1802 - val_accuracy: 0.5813\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 1.0281 - accuracy: 0.5832 - val_loss: 1.0297 - val_accuracy: 0.6012\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 1.0372 - accuracy: 0.5594 - val_loss: 1.2527 - val_accuracy: 0.4387\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 1.0724 - accuracy: 0.5383 - val_loss: 1.3644 - val_accuracy: 0.5350\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 1.0228 - accuracy: 0.5725 - val_loss: 1.1621 - val_accuracy: 0.4750\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.9954 - accuracy: 0.6017 - val_loss: 1.5867 - val_accuracy: 0.4300\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.0362 - accuracy: 0.5765 - val_loss: 1.0221 - val_accuracy: 0.6438\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 1.0158 - accuracy: 0.5667 - val_loss: 1.0569 - val_accuracy: 0.6000\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.9935 - accuracy: 0.5831 - val_loss: 1.1004 - val_accuracy: 0.6025\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 1.0112 - accuracy: 0.5920 - val_loss: 1.0807 - val_accuracy: 0.5763\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 1.0073 - accuracy: 0.5870 - val_loss: 1.2631 - val_accuracy: 0.5562\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.9939 - accuracy: 0.6039 - val_loss: 0.9121 - val_accuracy: 0.6612\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 0.9644 - accuracy: 0.6086 - val_loss: 1.0195 - val_accuracy: 0.6363\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.9455 - accuracy: 0.6314 - val_loss: 0.9336 - val_accuracy: 0.6550\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.9510 - accuracy: 0.6185 - val_loss: 1.1980 - val_accuracy: 0.5663\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.9395 - accuracy: 0.6302 - val_loss: 1.3861 - val_accuracy: 0.4525\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 1.1201 - accuracy: 0.5363 - val_loss: 1.0110 - val_accuracy: 0.6112\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 2s 90ms/step - loss: 0.9931 - accuracy: 0.6127 - val_loss: 1.0310 - val_accuracy: 0.6225\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.9534 - accuracy: 0.6158 - val_loss: 1.1074 - val_accuracy: 0.6000\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.9488 - accuracy: 0.6199 - val_loss: 1.2788 - val_accuracy: 0.5475\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 0.9098 - accuracy: 0.6373 - val_loss: 1.1231 - val_accuracy: 0.6075\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 0.8977 - accuracy: 0.6573 - val_loss: 1.0223 - val_accuracy: 0.6463\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.9114 - accuracy: 0.6401 - val_loss: 1.0599 - val_accuracy: 0.6450\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.9367 - accuracy: 0.6348 - val_loss: 1.2156 - val_accuracy: 0.5888\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.8905 - accuracy: 0.6499 - val_loss: 1.1554 - val_accuracy: 0.5825\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.9169 - accuracy: 0.6335 - val_loss: 1.1451 - val_accuracy: 0.5938\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.9065 - accuracy: 0.6253 - val_loss: 0.8766 - val_accuracy: 0.6662\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.9099 - accuracy: 0.6613 - val_loss: 0.9395 - val_accuracy: 0.6288\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 1.0002 - accuracy: 0.5874 - val_loss: 0.8374 - val_accuracy: 0.7300\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 0.9461 - accuracy: 0.6329 - val_loss: 0.9450 - val_accuracy: 0.6612\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.8764 - accuracy: 0.6605 - val_loss: 0.8714 - val_accuracy: 0.6438\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 0.9296 - accuracy: 0.6313 - val_loss: 0.9103 - val_accuracy: 0.6450\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 0.8783 - accuracy: 0.6580 - val_loss: 0.9590 - val_accuracy: 0.6475\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.9067 - accuracy: 0.6567 - val_loss: 1.0795 - val_accuracy: 0.5113\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.8790 - accuracy: 0.6402 - val_loss: 1.0590 - val_accuracy: 0.5788\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.9561 - accuracy: 0.6159 - val_loss: 0.9302 - val_accuracy: 0.6562\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 0.8855 - accuracy: 0.6468 - val_loss: 1.1255 - val_accuracy: 0.6050\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 0.9002 - accuracy: 0.6385 - val_loss: 0.8846 - val_accuracy: 0.6450\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 0.8508 - accuracy: 0.6612 - val_loss: 0.8063 - val_accuracy: 0.6913\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.8894 - accuracy: 0.6448 - val_loss: 1.2094 - val_accuracy: 0.6037\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 0.8480 - accuracy: 0.6678 - val_loss: 1.0418 - val_accuracy: 0.6425\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.9340 - accuracy: 0.6211 - val_loss: 1.1939 - val_accuracy: 0.6212\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 0.9424 - accuracy: 0.6085 - val_loss: 1.3702 - val_accuracy: 0.5312\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 0.8931 - accuracy: 0.6428 - val_loss: 1.0401 - val_accuracy: 0.6237\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.8666 - accuracy: 0.6395 - val_loss: 1.1296 - val_accuracy: 0.5913\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.8710 - accuracy: 0.6621 - val_loss: 0.9727 - val_accuracy: 0.6587\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 0.8396 - accuracy: 0.6718 - val_loss: 1.0284 - val_accuracy: 0.6538\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 0.8289 - accuracy: 0.6743 - val_loss: 1.0563 - val_accuracy: 0.6250\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 0.8600 - accuracy: 0.6639 - val_loss: 1.0925 - val_accuracy: 0.6100\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.8535 - accuracy: 0.6646 - val_loss: 1.0731 - val_accuracy: 0.6075\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.8863 - accuracy: 0.6524 - val_loss: 1.1525 - val_accuracy: 0.6212\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.8876 - accuracy: 0.6513 - val_loss: 0.9994 - val_accuracy: 0.6263\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 1.0057 - accuracy: 0.5833 - val_loss: 1.1474 - val_accuracy: 0.5038\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 0.8847 - accuracy: 0.6420 - val_loss: 1.0168 - val_accuracy: 0.6313\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 0.8194 - accuracy: 0.6736 - val_loss: 0.9815 - val_accuracy: 0.6150\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.9413 - accuracy: 0.6100 - val_loss: 1.1424 - val_accuracy: 0.6025\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 2s 129ms/step - loss: 0.8594 - accuracy: 0.6543 - val_loss: 0.9195 - val_accuracy: 0.6500\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.8183 - accuracy: 0.6767 - val_loss: 1.0424 - val_accuracy: 0.6350\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.7958 - accuracy: 0.7026 - val_loss: 0.9319 - val_accuracy: 0.6400\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.7972 - accuracy: 0.6979 - val_loss: 0.9185 - val_accuracy: 0.6513\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.9347 - accuracy: 0.6294 - val_loss: 1.2293 - val_accuracy: 0.5750\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 0.8518 - accuracy: 0.6676 - val_loss: 1.0632 - val_accuracy: 0.6062\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.8049 - accuracy: 0.6875 - val_loss: 0.6965 - val_accuracy: 0.7312\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.8492 - accuracy: 0.6594 - val_loss: 0.9414 - val_accuracy: 0.6575\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.8433 - accuracy: 0.6822 - val_loss: 1.2186 - val_accuracy: 0.5813\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.8522 - accuracy: 0.6697 - val_loss: 0.9890 - val_accuracy: 0.6175\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.8533 - accuracy: 0.6636 - val_loss: 1.0950 - val_accuracy: 0.5987\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.8548 - accuracy: 0.6712 - val_loss: 0.9395 - val_accuracy: 0.6612\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.8136 - accuracy: 0.6816 - val_loss: 1.0160 - val_accuracy: 0.6500\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 0.8329 - accuracy: 0.6698 - val_loss: 1.2562 - val_accuracy: 0.5600\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 0.8515 - accuracy: 0.6597 - val_loss: 0.9809 - val_accuracy: 0.6625\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.8477 - accuracy: 0.6706 - val_loss: 0.8718 - val_accuracy: 0.6625\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.8490 - accuracy: 0.6548 - val_loss: 1.0418 - val_accuracy: 0.6300\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.8404 - accuracy: 0.6654 - val_loss: 1.1596 - val_accuracy: 0.6012\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.8553 - accuracy: 0.6588 - val_loss: 0.9365 - val_accuracy: 0.6500\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 0.8298 - accuracy: 0.6730 - val_loss: 1.1868 - val_accuracy: 0.5688\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.8733 - accuracy: 0.6540 - val_loss: 1.0642 - val_accuracy: 0.6012\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.8410 - accuracy: 0.6656 - val_loss: 1.0722 - val_accuracy: 0.6025\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.8357 - accuracy: 0.6756 - val_loss: 1.0885 - val_accuracy: 0.6075\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.8020 - accuracy: 0.6809 - val_loss: 1.0509 - val_accuracy: 0.6338\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.8202 - accuracy: 0.6736 - val_loss: 0.8434 - val_accuracy: 0.6812\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.8208 - accuracy: 0.6804 - val_loss: 0.9256 - val_accuracy: 0.6475\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.8245 - accuracy: 0.6746 - val_loss: 1.0705 - val_accuracy: 0.6288\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.8144 - accuracy: 0.6744 - val_loss: 1.0247 - val_accuracy: 0.6475\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.8006 - accuracy: 0.6932 - val_loss: 1.0309 - val_accuracy: 0.6400\n",
      "Epoch 115/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7911 - accuracy: 0.6867 - val_loss: 1.3102 - val_accuracy: 0.5850\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7803 - accuracy: 0.7124 - val_loss: 0.8037 - val_accuracy: 0.6787\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7759 - accuracy: 0.6996 - val_loss: 0.8061 - val_accuracy: 0.6950\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.7552 - accuracy: 0.6984 - val_loss: 1.0835 - val_accuracy: 0.6225\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7737 - accuracy: 0.6984 - val_loss: 0.9913 - val_accuracy: 0.6475\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7593 - accuracy: 0.7029 - val_loss: 1.1130 - val_accuracy: 0.6263\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7488 - accuracy: 0.7187 - val_loss: 0.7922 - val_accuracy: 0.7050\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7702 - accuracy: 0.6957 - val_loss: 1.2753 - val_accuracy: 0.5950\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.8219 - accuracy: 0.6744 - val_loss: 0.8645 - val_accuracy: 0.6662\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7890 - accuracy: 0.6878 - val_loss: 0.9037 - val_accuracy: 0.6787\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7686 - accuracy: 0.7073 - val_loss: 1.2540 - val_accuracy: 0.5788\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7389 - accuracy: 0.7161 - val_loss: 0.9479 - val_accuracy: 0.6500\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7380 - accuracy: 0.7185 - val_loss: 1.0873 - val_accuracy: 0.6275\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7338 - accuracy: 0.7306 - val_loss: 0.9482 - val_accuracy: 0.6575\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7756 - accuracy: 0.6991 - val_loss: 1.2097 - val_accuracy: 0.6050\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7471 - accuracy: 0.7106 - val_loss: 1.1324 - val_accuracy: 0.6225\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7368 - accuracy: 0.7142 - val_loss: 1.1053 - val_accuracy: 0.6237\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7466 - accuracy: 0.7129 - val_loss: 1.0044 - val_accuracy: 0.6562\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.7278 - accuracy: 0.7152 - val_loss: 1.0543 - val_accuracy: 0.6413\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.6989 - accuracy: 0.7384 - val_loss: 1.0192 - val_accuracy: 0.6575\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7670 - accuracy: 0.6969 - val_loss: 1.0437 - val_accuracy: 0.6363\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7944 - accuracy: 0.6810 - val_loss: 0.9163 - val_accuracy: 0.6725\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7333 - accuracy: 0.7120 - val_loss: 0.6561 - val_accuracy: 0.7525\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.8416 - accuracy: 0.6757 - val_loss: 1.0224 - val_accuracy: 0.6187\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.8143 - accuracy: 0.6801 - val_loss: 1.1456 - val_accuracy: 0.6075\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7941 - accuracy: 0.7010 - val_loss: 1.1027 - val_accuracy: 0.6037\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.7819 - accuracy: 0.7011 - val_loss: 0.9649 - val_accuracy: 0.6525\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7389 - accuracy: 0.7243 - val_loss: 1.0869 - val_accuracy: 0.6313\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7541 - accuracy: 0.7216 - val_loss: 1.0071 - val_accuracy: 0.6413\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.7778 - accuracy: 0.7036 - val_loss: 0.9639 - val_accuracy: 0.6600\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.7576 - accuracy: 0.7098 - val_loss: 1.0478 - val_accuracy: 0.6237\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7405 - accuracy: 0.7140 - val_loss: 0.8473 - val_accuracy: 0.6812\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7293 - accuracy: 0.7134 - val_loss: 1.3225 - val_accuracy: 0.5800\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7514 - accuracy: 0.7128 - val_loss: 0.9378 - val_accuracy: 0.6700\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7148 - accuracy: 0.7246 - val_loss: 0.8791 - val_accuracy: 0.6862\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7490 - accuracy: 0.7058 - val_loss: 0.9321 - val_accuracy: 0.6700\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7285 - accuracy: 0.7197 - val_loss: 1.0425 - val_accuracy: 0.6325\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7543 - accuracy: 0.7147 - val_loss: 1.4698 - val_accuracy: 0.4812\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7953 - accuracy: 0.6888 - val_loss: 1.0621 - val_accuracy: 0.6212\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7542 - accuracy: 0.7217 - val_loss: 1.1568 - val_accuracy: 0.6125\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7343 - accuracy: 0.7175 - val_loss: 0.8120 - val_accuracy: 0.7013\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7300 - accuracy: 0.7313 - val_loss: 1.2044 - val_accuracy: 0.6200\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7225 - accuracy: 0.7261 - val_loss: 0.9431 - val_accuracy: 0.6525\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7584 - accuracy: 0.7058 - val_loss: 0.9965 - val_accuracy: 0.6263\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7557 - accuracy: 0.7131 - val_loss: 0.7219 - val_accuracy: 0.7250\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7851 - accuracy: 0.7037 - val_loss: 0.9723 - val_accuracy: 0.6575\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.8100 - accuracy: 0.6924 - val_loss: 0.8190 - val_accuracy: 0.6825\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7669 - accuracy: 0.7079 - val_loss: 0.8115 - val_accuracy: 0.6938\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 0.7396 - accuracy: 0.7179 - val_loss: 0.8532 - val_accuracy: 0.6913\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7023 - accuracy: 0.7357 - val_loss: 1.0413 - val_accuracy: 0.6350\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7186 - accuracy: 0.7245 - val_loss: 0.9985 - val_accuracy: 0.6562\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7547 - accuracy: 0.7085 - val_loss: 1.1760 - val_accuracy: 0.6162\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.7324 - accuracy: 0.7157 - val_loss: 0.8379 - val_accuracy: 0.6850\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7248 - accuracy: 0.7360 - val_loss: 1.1890 - val_accuracy: 0.6187\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7615 - accuracy: 0.7133 - val_loss: 1.0035 - val_accuracy: 0.6500\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7382 - accuracy: 0.7189 - val_loss: 0.8693 - val_accuracy: 0.6762\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7208 - accuracy: 0.7228 - val_loss: 0.7989 - val_accuracy: 0.7050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7016 - accuracy: 0.7419 - val_loss: 0.9603 - val_accuracy: 0.6575\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7087 - accuracy: 0.7287 - val_loss: 0.8290 - val_accuracy: 0.6938\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7469 - accuracy: 0.7199 - val_loss: 0.9759 - val_accuracy: 0.6488\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7047 - accuracy: 0.7236 - val_loss: 0.7953 - val_accuracy: 0.7038\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.6980 - accuracy: 0.7320 - val_loss: 0.8289 - val_accuracy: 0.6888\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7253 - accuracy: 0.7121 - val_loss: 0.8659 - val_accuracy: 0.6700\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7361 - accuracy: 0.7170 - val_loss: 0.9944 - val_accuracy: 0.6625\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.7644 - accuracy: 0.7094 - val_loss: 1.0627 - val_accuracy: 0.5675\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.9041 - accuracy: 0.6338 - val_loss: 0.9359 - val_accuracy: 0.6538\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.8110 - accuracy: 0.6910 - val_loss: 1.0087 - val_accuracy: 0.6350\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.7348 - accuracy: 0.7178 - val_loss: 0.9755 - val_accuracy: 0.6525\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7301 - accuracy: 0.7154 - val_loss: 0.9653 - val_accuracy: 0.6587\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7352 - accuracy: 0.7144 - val_loss: 0.8880 - val_accuracy: 0.6800\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.6987 - accuracy: 0.7403 - val_loss: 0.9072 - val_accuracy: 0.6825\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.6928 - accuracy: 0.7381 - val_loss: 1.2562 - val_accuracy: 0.5312\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7657 - accuracy: 0.7086 - val_loss: 1.1156 - val_accuracy: 0.5163\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7452 - accuracy: 0.7122 - val_loss: 0.9422 - val_accuracy: 0.6413\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7296 - accuracy: 0.7282 - val_loss: 0.7292 - val_accuracy: 0.7063\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7413 - accuracy: 0.7129 - val_loss: 0.6411 - val_accuracy: 0.7487\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7438 - accuracy: 0.7110 - val_loss: 1.0260 - val_accuracy: 0.6375\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7298 - accuracy: 0.7212 - val_loss: 0.7979 - val_accuracy: 0.7038\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.7260 - accuracy: 0.7272 - val_loss: 0.6923 - val_accuracy: 0.7375\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7902 - accuracy: 0.7013 - val_loss: 0.7342 - val_accuracy: 0.7125\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7572 - accuracy: 0.7032 - val_loss: 0.8104 - val_accuracy: 0.7025\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7291 - accuracy: 0.7280 - val_loss: 0.9098 - val_accuracy: 0.6750\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.6725 - accuracy: 0.7570 - val_loss: 0.7850 - val_accuracy: 0.7125\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.6749 - accuracy: 0.7420 - val_loss: 0.8051 - val_accuracy: 0.7025\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.6836 - accuracy: 0.7400 - val_loss: 0.8579 - val_accuracy: 0.7013\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.7442 - accuracy: 0.7230 - val_loss: 0.8386 - val_accuracy: 0.6900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20076aa5488>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow(x_train,y_train, batch_size=200, shuffle=True),\n",
    "                              epochs = 200, validation_data = (x_val,y_val),  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "36149ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.693"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_pred = model.predict(X_test)\n",
    "Y_true = np.argmax(Y_test, axis=1)\n",
    "accuracy_score(Y_true, np.argmax(Y_test_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9158eb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929b8d04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_keras",
   "language": "python",
   "name": "env_keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
